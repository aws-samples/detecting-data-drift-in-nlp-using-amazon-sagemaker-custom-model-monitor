{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Data Drift in NLP using SageMaker Custom Model Monitor\n",
    "\n",
    "This Example is an extension of [Fine-tuning a PyTorch BERT model and deploying it with Amazon Elastic Inference on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker/) aws blog post. We will use the dataset and model thats outlined in this blog and extend it to demo custom model monitoring capability using [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)\n",
    "\n",
    "Detecting data drift in NLP is a challenging task. Model monitoring becomes an important aspect in MLOPs because the change in data distribution during inference time can cause Model decay. ML models are probabilistic and trained on certain corpus of historical data. Drift is distribution change between the training and deployment data, which is concerning if model performance changes.\n",
    "\n",
    "We will begin with creating PyTorch Model using previously trained model artifacts. We will deploy the model to a SageMaker real time endpoint. To establish a baseline of training data distribution we will caluclate BERT sentence embedding and use that in the custom model monitoring scripts to compare the real time inferece traffic to compare a distance metrics to determine the deviation fron training distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/nlp-data-drift-bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To start, we import some Python libraries and initialize a SageMaker session, S3 bucket and prefix, and IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (21.2.4)\r\n"
     ]
    }
   ],
   "source": [
    "# need torch 1.3.1 for elastic inference\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install torch==1.3.1 --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "model_prefix = \"sagemaker/nlp-data-drift-bert-model\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "\n",
    "We use Corpus of Linguistic Acceptability (CoLA) (https://nyu-mll.github.io/CoLA/), a dataset of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. We download and unzip the data using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./cola_public_1.1.zip\"):\n",
    "    !curl -o ./cola_public_1.1.zip https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
    "if not os.path.exists(\"./cola_public/\"):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentences and labels\n",
    "\n",
    "Let us take a quick look at our data. First we read in the training data. The only two columns we need are the sentence itself and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./cola_public/raw/in_domain_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Our friends won't buy this analysis, let alone...\n",
       "1      1  One more pseudo generalization and I'm giving up.\n",
       "2      1   One more pseudo generalization or I'm giving up.\n",
       "3      1     The more we study verbs, the crazier they get.\n",
       "4      1          Day by day the facts are getting murkier."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out a few sentences shows us how sentences are labeled based on their grammatical completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The professor talked us.' 'We yelled ourselves hoarse.'\n",
      " 'We yelled ourselves.' 'We yelled Harry hoarse.'\n",
      " 'Harry coughed himself into a fit.']\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[20:25])\n",
    "print(labels[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "train.to_csv(\"./cola_public/train.csv\", index=False)\n",
    "test.to_csv(\"./cola_public/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we upload both to Amazon S3 for use later. The SageMaker Python SDK provides a helpful function for uploading to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./cola_public/train.csv\", bucket=bucket, key_prefix=model_prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"./cola_public/test.csv\", bucket=bucket, key_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script\n",
    "\n",
    "We use the [PyTorch-Transformers library](https://pytorch.org/hub/huggingface_pytorch-transformers), which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called `model_dir`, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in `model_dir` will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named `train_deploy.py`, and put the file in a directory named `code/`. The full training script can be viewed under `code/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader, RandomSampler, TensorDataset\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AdamW, BertForSequenceClassification, BertTokenizer\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "MAX_LEN = \u001b[34m64\u001b[39;49;00m  \u001b[37m# this is the max length of the sentence\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading BERT tokenizer...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, do_lower_case=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mflat_accuracy\u001b[39;49;00m(preds, labels):\r\n",
      "    pred_flat = np.argmax(preds, axis=\u001b[34m1\u001b[39;49;00m).flatten()\r\n",
      "    labels_flat = labels.flatten()\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m np.sum(pred_flat == labels_flat) / \u001b[36mlen\u001b[39;49;00m(labels_flat)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    sentences = dataset.sentence.values\r\n",
      "    labels = dataset.label.values\r\n",
      "\r\n",
      "    input_ids = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\r\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        input_ids.append(encoded_sent)\r\n",
      "\r\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\r\n",
      "    input_ids_padded = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\r\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        input_ids_padded.append(i)\r\n",
      "    input_ids = input_ids_padded\r\n",
      "\r\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\r\n",
      "    attention_masks = []\r\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\r\n",
      "        attention_masks.append(att_mask)\r\n",
      "\r\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\r\n",
      "    train_inputs = torch.tensor(input_ids)\r\n",
      "    train_labels = torch.tensor(labels)\r\n",
      "    train_masks = torch.tensor(attention_masks)\r\n",
      "\r\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        train_sampler = RandomSampler(train_data)\r\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir):\r\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "    sentences = dataset.sentence.values\r\n",
      "    labels = dataset.label.values\r\n",
      "\r\n",
      "    input_ids = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m sentences:\r\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        input_ids.append(encoded_sent)\r\n",
      "\r\n",
      "    \u001b[37m# pad shorter sentences\u001b[39;49;00m\r\n",
      "    input_ids_padded = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(i) < MAX_LEN:\r\n",
      "            i.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        input_ids_padded.append(i)\r\n",
      "    input_ids = input_ids_padded\r\n",
      "\r\n",
      "    \u001b[37m# mask; 0: added, 1: otherwise\u001b[39;49;00m\r\n",
      "    attention_masks = []\r\n",
      "    \u001b[37m# For each sentence...\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m sent \u001b[35min\u001b[39;49;00m input_ids:\r\n",
      "        att_mask = [\u001b[36mint\u001b[39;49;00m(token_id > \u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token_id \u001b[35min\u001b[39;49;00m sent]\r\n",
      "        attention_masks.append(att_mask)\r\n",
      "\r\n",
      "    \u001b[37m# convert to PyTorch data types.\u001b[39;49;00m\r\n",
      "    train_inputs = torch.tensor(input_ids)\r\n",
      "    train_labels = torch.tensor(labels)\r\n",
      "    train_masks = torch.tensor(attention_masks)\r\n",
      "\r\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
      "    train_sampler = RandomSampler(train_data)\r\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataloader\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, is_distributed)\r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args.num_gpus)\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\r\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\r\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        logger.info(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            args.backend, dist.get_world_size(),\r\n",
      "            dist.get_rank(), args.num_gpus\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed)\r\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.debug(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.sampler),\r\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\r\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = BertForSequenceClassification.from_pretrained(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m# Use the 12-layer BERT model, with an uncased vocab.\u001b[39;49;00m\r\n",
      "        num_labels=args.num_labels,  \u001b[37m# The number of output labels--2 for binary classification.\u001b[39;49;00m\r\n",
      "        output_attentions=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns attentions weights.\u001b[39;49;00m\r\n",
      "        output_hidden_states=\u001b[34mFalse\u001b[39;49;00m,  \u001b[37m# Whether the model returns all hidden-states.\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    model = model.to(device)\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\r\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\r\n",
      "        model = torch.nn.DataParallel(model)\r\n",
      "    optimizer = AdamW(\r\n",
      "        model.parameters(),\r\n",
      "        lr=\u001b[34m2e-5\u001b[39;49;00m,  \u001b[37m# args.learning_rate - default is 5e-5, our notebook had 2e-5\u001b[39;49;00m\r\n",
      "        eps=\u001b[34m1e-8\u001b[39;49;00m,  \u001b[37m# args.adam_epsilon - default is 1e-8.\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnd of defining BertForSequenceClassification\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "        model.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m step, batch \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\r\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\r\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\r\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\r\n",
      "            model.zero_grad()\r\n",
      "\r\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask, labels=b_labels)\r\n",
      "            loss = outputs[\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "            total_loss += loss.item()\r\n",
      "            loss.backward()\r\n",
      "            torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[34m1.0\u001b[39;49;00m)\r\n",
      "            \u001b[37m# modified based on their gradients, the learning rate, etc.\u001b[39;49;00m\r\n",
      "            optimizer.step()\r\n",
      "            \u001b[34mif\u001b[39;49;00m step % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                logger.info(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        step * \u001b[36mlen\u001b[39;49;00m(batch[\u001b[34m0\u001b[39;49;00m]),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * step / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mAverage training loss: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader))\r\n",
      "\r\n",
      "        test(model, test_loader, device)\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving tuned model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model_2_save = model.module \u001b[34mif\u001b[39;49;00m \u001b[36mhasattr\u001b[39;49;00m(model, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodule\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34melse\u001b[39;49;00m model\r\n",
      "    model_2_save.save_pretrained(save_directory=args.model_dir)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\r\n",
      "    model.eval()\r\n",
      "    _, eval_accuracy = \u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            b_input_ids = batch[\u001b[34m0\u001b[39;49;00m].to(device)\r\n",
      "            b_input_mask = batch[\u001b[34m1\u001b[39;49;00m].to(device)\r\n",
      "            b_labels = batch[\u001b[34m2\u001b[39;49;00m].to(device)\r\n",
      "\r\n",
      "            outputs = model(b_input_ids, token_type_ids=\u001b[34mNone\u001b[39;49;00m, attention_mask=b_input_mask)\r\n",
      "            logits = outputs[\u001b[34m0\u001b[39;49;00m]\r\n",
      "            logits = logits.detach().cpu().numpy()\r\n",
      "            label_ids = b_labels.to(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).numpy()\r\n",
      "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
      "            eval_accuracy += tmp_eval_accuracy\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Accuracy: \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, tmp_eval_accuracy)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ objects in model_dir ===================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.listdir(model_dir))\r\n",
      "    model = BertForSequenceClassification.from_pretrained(model_dir)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ model loaded ===========================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\r\n",
      "    \u001b[33m\"\"\"An input_fn that loads a pickled tensor\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        data = json.loads(request_body)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ input sentences ===============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(data)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mstr\u001b[39;49;00m):\r\n",
      "            data = [data]\r\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mlist\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(data) > \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data[\u001b[34m0\u001b[39;49;00m], \u001b[36mstr\u001b[39;49;00m):\r\n",
      "            \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported input type. Input type can be a string or an non-empty list. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m                             I got \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data))\r\n",
      "                       \r\n",
      "        \u001b[37m#encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\u001b[39;49;00m\r\n",
      "        \u001b[37m#encoded = tokenizer(data, add_special_tokens=True) \u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# for backward compatibility use the following way to encode \u001b[39;49;00m\r\n",
      "        \u001b[37m# https://github.com/huggingface/transformers/issues/5580\u001b[39;49;00m\r\n",
      "        input_ids = [tokenizer.encode(x, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m data]\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ encoded sentences ==============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(input_ids)\r\n",
      "\r\n",
      "        \u001b[37m# pad shorter sentence\u001b[39;49;00m\r\n",
      "        padded =  torch.zeros(\u001b[36mlen\u001b[39;49;00m(input_ids), MAX_LEN) \r\n",
      "        \u001b[34mfor\u001b[39;49;00m i, p \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(input_ids):\r\n",
      "            padded[i, :\u001b[36mlen\u001b[39;49;00m(p)] = torch.tensor(p)\r\n",
      "     \r\n",
      "        \u001b[37m# create mask\u001b[39;49;00m\r\n",
      "        mask = (padded != \u001b[34m0\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================= padded input and attention mask ================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(padded, \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mask)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m padded.long(), mask.long()\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported content type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(request_content_type))\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model.to(device)\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    input_id, input_mask = input_data\r\n",
      "    input_id = input_id.to(device)\r\n",
      "    input_mask = input_mask.to(device)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m============== encoded data =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(input_id, input_mask)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        y = model(input_id, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=============== inference result =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(y)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m y\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--num_labels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m50\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train(parser.parse_args())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train_deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Amazon SageMaker\n",
    "\n",
    "We use Amazon SageMaker to train and deploy a model using our custom PyTorch code. The Amazon SageMaker Python SDK makes it easier to run a PyTorch script in Amazon SageMaker using its PyTorch estimator. After that, we can use the SageMaker Python SDK to deploy the trained model and run predictions. For more information on how to use this SDK with PyTorch, see [the SageMaker Python SDK documentation](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html).\n",
    "\n",
    "To start, we use the `PyTorch` estimator class to train our model. When creating our estimator, we make sure to specify a few things:\n",
    "\n",
    "* `entry_point`: the name of our PyTorch script. It contains our training script, which loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. It also contains code to load and run the model during inference.\n",
    "* `source_dir`: the location of our training scripts and requirements.txt file. \"requirements.txt\" lists packages you want to use with your script.\n",
    "* `framework_version`: the PyTorch version we want to use\n",
    "\n",
    "The PyTorch estimator supports multi-machine, distributed PyTorch training. To use this, we just set train_instance_count to be greater than one. Our training script supports distributed training for only GPU instances. \n",
    "\n",
    "After creating the estimator, we then call fit(), which launches a training job. We use the Amazon S3 URIs where we uploaded the training data earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 21:14:46 Starting - Starting the training job...\n",
      "2021-08-12 21:14:49 Starting - Launching requested ML instances......\n",
      "2021-08-12 21:16:04 Starting - Preparing the instances for training............\n",
      "2021-08-12 21:17:53 Downloading - Downloading input data...\n",
      "2021-08-12 21:18:26 Training - Downloading the training image......\n",
      "2021-08-12 21:19:45 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:46,535 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:46,561 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:49,576 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:49,888 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:49,888 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:49,888 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:49,889 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp2x0l59a8/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2021.8.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.11.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=12024 sha256=2b0bc8a8137e5d23c754113286bae11bfd3ceceefda0cc804a38d95c6bf06eb6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s8cak7z3/wheels/50/bc/2f/a68922e9daa1ca513d04f6773e6864086329158e5c67465cf0\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sentencepiece, sacremoses, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 regex-2021.8.3 sacremoses-0.0.45 sentencepiece-0.1.96 transformers-2.3.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 21.2.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-08-12 21:19:54,289 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-08-12-21-14-45-772\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-757967535041/pytorch-training-2021-08-12-21-14-45-772/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-757967535041/pytorch-training-2021-08-12-21-14-45-772/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-08-12-21-14-45-772\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-757967535041/pytorch-training-2021-08-12-21-14-45-772/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_deploy.py --backend gloo --epochs 1 --num_labels 2\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/6413 (0%)] Loss: 0.632982\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/6413 (50%)] Loss: 0.448342\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1300/6413 (99%)] Loss: 0.205874\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.509086\u001b[0m\n",
      "\u001b[34mProcesses 6413/6413 (100%) of train data\n",
      "\u001b[0m\n",
      "\u001b[34mProcesses 2138/2138 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.768116\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mINFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:transformers.modeling_utils:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:16.185 algo-1:46 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:16.186 algo-1:46 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:16.186 algo-1:46 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:16.201 algo-1:46 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.020 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.020 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.020 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.020 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.027 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.027 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.027 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.030 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.030 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.030 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.031 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.033 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.033 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.033 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.036 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.036 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.037 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.037 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.039 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.040 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.040 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.043 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.043 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.043 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.044 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.046 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.046 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.046 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.049 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.049 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.049 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.050 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.052 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.052 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.052 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.055 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.056 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.056 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.056 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.059 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.059 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.059 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.062 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.062 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.062 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.063 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.065 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.065 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.065 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.068 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.068 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.069 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.069 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.071 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.072 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.072 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.075 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.075 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.075 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.075 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.078 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.078 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.078 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.081 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.081 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.081 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.082 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.084 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.084 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.084 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.088 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.088 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.088 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.088 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.091 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.091 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.091 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.094 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.094 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.094 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.095 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.097 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.097 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:17.097 algo-1:46 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/6413 (0%)] Loss: 0.632982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/6413 (50%)] Loss: 0.448342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1300/6413 (99%)] Loss: 0.205874\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.509086\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.768116\n",
      "\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34m[2021-08-12 21:20:46.976 algo-1:46 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2021-08-12 21:20:47,483 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-08-12 21:20:52 Uploading - Uploading generated training model\n",
      "2021-08-12 21:21:50 Completed - Training job completed\n",
      "Training seconds: 237\n",
      "Billable seconds: 237\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{model_prefix}\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,  # this script only support distributed training for GPU instances.\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True, # disable debugger\n",
    ")\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we host it on an Amazon SageMaker Endpoint. To make the endpoint load the model and serve predictions, we implement a few methods in `train_deploy.py`.\n",
    "\n",
    "* `model_fn()`: function defined to load the saved model and return a model object that can be used for model serving. The SageMaker PyTorch model server loads our model by invoking model_fn.\n",
    "* `input_fn()`: deserializes and prepares the prediction input. In this example, our request body is first serialized to JSON and then sent to model serving endpoint. Therefore, in `input_fn()`, we first deserialize the JSON-formatted request body and return the input as a `torch.tensor`, as required for BERT.\n",
    "* `predict_fn()`: performs the prediction and returns the result.\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our PyTorch estimator object, passing in our desired number of instances and instance type:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Model Monitor Data Capture on the SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "#s3_capture_upload_path = f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture'\n",
    "prefix = \"sagemaker/CustomModelMonitor\"\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_upload_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(endpoint_name='nlp-data-drift-bert-endpoint',\n",
    "                             initial_instance_count=1, \n",
    "                             instance_type=\"ml.m4.xlarge\",\n",
    "                             data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-data-drift-bert-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint nlp-data-drift-bert-endpoint. \n",
      "Please wait...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# batch inference \n",
    "\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "result = predictor.predict([\n",
    "    \"CLI to download the zip file\", \n",
    "    \"Thanks so much for driving me home\",\n",
    "    \"construct the sub-embeddings and corresponding baselines\",\n",
    "    \"our Bert model and interpret what the model\",\n",
    "    \"Bert models using Captum library\",\n",
    "    \"case study we focus on a fine-tuned Question Answering model on SQUAD datase\",\n",
    "    \"we pretrain the model, we can load \",\n",
    "    \"need to define baselines / references, nu\",\n",
    "    \"defines numericalized special tokens \",\n",
    "    \"Thanks so much for cooking dinner. I really appreciate it\",\n",
    "    \"let's define the ground truth for prediction's start and en\",\n",
    "    \"pre-computation of embeddings for the second option is necessary because\",\n",
    "    \"to summarize attributions for each word token in the sequence.\",\n",
    "    \"Nice to meet you, Sergio. So, where are you from\"\n",
    "])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  [0 1 1 0 1 1 1 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class: \", np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "sagemaker/CustomModelMonitor/datacapture/nlp-data-drift-bert-endpoint/AllTraffic/2021/08/12/21/32-28-582-0dafe95a-370b-43d4-85d0-345a952d8ac7.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Note: It takes a few minutes for the capture data to appear in S3\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"application/json\",\"mode\":\"INPUT\",\"data\":\"[\\\"CLI to download the zip file\\\", \\\"Thanks so much for driving me home\\\", \\\"construct the sub-embeddings and corresponding baselines\\\", \\\"our Bert model and interpret what the model\\\", \\\"Bert models using Captum library\\\", \\\"case study we focus on a fine-tuned Question Answering model on SQUAD datase\\\", \\\"we pretrain the model, we can load \\\", \\\"need to define baselines / references, nu\\\", \\\"defines numericalized special tokens \\\", \\\"Thanks so much for cooking dinner. I really appreciate it\\\", \\\"let's define the ground truth for prediction's start and en\\\", \\\"pre-computation of embeddings for the second option is necessary because\\\", \\\"to summarize attributions for each word token in the sequence.\\\", \\\"Nice to meet you, Sergio. So, where are you from\\\"]\",\"encoding\":\"JSON\"},\"endpointOutput\":{\"observedContentType\":\"application/json\",\"mode\":\"OUTPUT\",\"data\":\"[[0.5500634908676147, -0.6241919994354248], [-0.9085295796394348, 1.7943570613861084], [-1.009690761566162, 1.5033482313156128], [0.5463818311691284, -0.19392339885234833], [0.09972967207431793, 0.3375684916973114], [0.09731347113847733, 0.5307489037513733], [-0.5267322063446045, 1.2601779699325562], [0.3630158305168152, 0.059125736355781555], [-0.7341494560241699, 1.425351619720459], [-0.8884592056274414, 1.7991210222244263], [0.47318652272224426, 0.05803534388542175], [-0.7489925026893616, 1.6023170948028564], [-0.6623589992523193, 1.410546064376831], [-0.9889964461326599, 1.7153725624084473]]\",\"encoding\":\"JSON\"}},\"eventMetadata\":{\"eventId\":\"cf6e6577-1937-4c59-bebd-8d5efd490b47\",\"inferenceTime\":\"2021-08-12T21:32:28Z\"},\"eventVersion\":\"0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"application/json\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"[\\\"CLI to download the zip file\\\", \\\"Thanks so much for driving me home\\\", \\\"construct the sub-embeddings and corresponding baselines\\\", \\\"our Bert model and interpret what the model\\\", \\\"Bert models using Captum library\\\", \\\"case study we focus on a fine-tuned Question Answering model on SQUAD datase\\\", \\\"we pretrain the model, we can load \\\", \\\"need to define baselines / references, nu\\\", \\\"defines numericalized special tokens \\\", \\\"Thanks so much for cooking dinner. I really appreciate it\\\", \\\"let's define the ground truth for prediction's start and en\\\", \\\"pre-computation of embeddings for the second option is necessary because\\\", \\\"to summarize attributions for each word token in the sequence.\\\", \\\"Nice to meet you, Sergio. So, where are you from\\\"]\",\n",
      "      \"encoding\": \"JSON\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"application/json\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"[[0.5500634908676147, -0.6241919994354248], [-0.9085295796394348, 1.7943570613861084], [-1.009690761566162, 1.5033482313156128], [0.5463818311691284, -0.19392339885234833], [0.09972967207431793, 0.3375684916973114], [0.09731347113847733, 0.5307489037513733], [-0.5267322063446045, 1.2601779699325562], [0.3630158305168152, 0.059125736355781555], [-0.7341494560241699, 1.425351619720459], [-0.8884592056274414, 1.7991210222244263], [0.47318652272224426, 0.05803534388542175], [-0.7489925026893616, 1.6023170948028564], [-0.6623589992523193, 1.410546064376831], [-0.9889964461326599, 1.7153725624084473]]\",\n",
      "      \"encoding\": \"JSON\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"cf6e6577-1937-4c59-bebd-8d5efd490b47\",\n",
      "    \"inferenceTime\": \"2021-08-12T21:32:28Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Embedding Matrix from Input Dataset\n",
    "\n",
    "BERT developers created two main models:\n",
    "\n",
    "    The BASE: Number of transformer blocks (L): 12, Hidden layer size (H): 768 and Attention heads(A): 12\n",
    "    The LARGE: Number of transformer blocks (L): 24, Hidden layer size (H): 1024 and Attention heads(A): 16\n",
    "\n",
    "In this post, I will be using the BASE model as it is smaller and easier to train.\n",
    "\n",
    "At this point, to make things clearer it is important to understand the special tokens that BERT authors used for fine-tuning and specific task training. These are the following:\n",
    "\n",
    "    [CLS] : The first token of every sequence. A classification token which is normally used in conjunction with a softmax layer for classification tasks. For anything else, it can be safely ignored.\n",
    "    [SEP] : A sequence delimiter token which was used at pre-training for sequence-pair tasks (i.e. Next sentence prediction). Must be used when sequence pair tasks are required. When a single sequence is used it is just appended at the end.\n",
    "    [MASK] : Token used for masked words. Only used for pre-training.\n",
    "    \n",
    "The input layer is simply the vector of the sequence tokens along with the special tokens. The ##ing token in the example above may raise some eyebrows so to clarify, BERT utilizes WordPiece [6] for tokenization which in effect, splits token like playing to play and ##ing. This is mainly to cover a wider spectrum of Out-Of-Vocabulary (OOV) words.\n",
    "\n",
    "Token embeddings are the vocabulary IDs for each of the tokens.\n",
    "\n",
    "Sentence Embeddings is just a numeric class to distinguish between sentence A and B.\n",
    "\n",
    "And lastly, Transformer positional embeddings indicate the position of each word in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])\n",
    "        hidden_states = outputs[2]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        sentence_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_list = []\n",
    "\n",
    "for i in sentence_embeddings:\n",
    "    sentence_embeddings_list.append(i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the embeddingd as .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', sentence_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.41458297e-01, -9.18637514e-02,  5.19061983e-01,  4.00024861e-01,\n",
       "        8.66516009e-02, -3.25883001e-01, -1.24016896e-01,  7.44160190e-02,\n",
       "        7.17647552e-01, -2.21916839e-01,  1.27307057e-01,  2.71159828e-01,\n",
       "       -2.26744950e-01,  3.91899735e-01, -1.14975095e-01, -1.54685304e-01,\n",
       "        5.66386938e-01,  1.60328206e-02,  5.11664748e-02, -8.28863606e-02,\n",
       "       -2.42579542e-02, -5.17325550e-02, -4.64936376e-01, -4.92869839e-02,\n",
       "        3.78277659e-01, -2.42931679e-01,  1.66605115e-02,  4.27487679e-02,\n",
       "       -4.87603545e-01,  3.37469906e-01,  1.70970187e-01, -4.77718234e-01,\n",
       "       -3.13292176e-01, -5.75484056e-03, -1.34845957e-01,  2.92394936e-01,\n",
       "       -2.04022348e-01,  4.02555823e-01, -5.67178726e-01,  1.42273039e-01,\n",
       "       -3.88651818e-01, -3.48770291e-01,  1.54633090e-01, -4.36054543e-02,\n",
       "       -4.35700864e-01, -2.95430809e-01,  3.90958995e-01,  2.03553393e-01,\n",
       "       -1.27091154e-01, -5.96861839e-01, -1.63994476e-01,  2.14198381e-01,\n",
       "       -1.01402458e-02,  8.75985250e-03,  2.94220179e-01, -3.91344577e-01,\n",
       "        1.33358613e-01, -8.47702324e-01, -4.13923293e-01,  1.51814297e-01,\n",
       "       -1.58178270e-01, -4.21443284e-02,  1.75296217e-01, -2.07068130e-01,\n",
       "        5.35203218e-01, -3.15407693e-01,  1.93224609e-01,  4.17491496e-01,\n",
       "       -8.05273473e-01, -4.79026377e-01, -2.67773539e-01, -4.22300026e-03,\n",
       "       -2.52414137e-01, -2.35971168e-01,  4.43621129e-01, -6.26237988e-02,\n",
       "       -4.15945321e-01, -9.00304839e-02, -4.09821272e-02, -3.11061502e-01,\n",
       "        1.05364338e-01,  2.92115301e-01, -4.86183882e-01, -5.59929870e-02,\n",
       "       -2.66968429e-01,  1.34093672e-01, -2.43663602e-03,  5.54834008e-01,\n",
       "       -3.76184225e-01,  6.04660869e-01, -3.53415877e-01,  8.36773738e-02,\n",
       "        3.47530514e-01, -1.01595573e-01,  2.04312846e-01, -4.65236843e-01,\n",
       "        1.46545529e-01, -1.96494404e-02, -3.63288254e-01, -2.95562595e-01,\n",
       "        2.18221650e-01, -1.08973277e+00, -4.16822582e-01, -5.00444956e-02,\n",
       "       -1.49681931e-02, -7.36219212e-02, -1.15439445e-01, -6.44769430e-01,\n",
       "       -1.18192703e-01, -1.35655642e-01,  2.10322142e-01,  5.56678355e-01,\n",
       "        1.35682642e-01, -6.99613035e-01, -6.73530549e-02,  3.08774978e-01,\n",
       "       -8.05479825e-01, -6.15152657e-01,  1.32988200e-01, -2.37497643e-01,\n",
       "        3.42423677e-01,  3.65608484e-02,  1.29980057e-01, -1.85523741e-02,\n",
       "       -1.68662325e-01, -1.33240551e-01, -3.03534508e-01,  1.23396918e-01,\n",
       "       -1.94968447e-01, -7.06585050e-01,  2.91217268e-01,  2.02377737e-01,\n",
       "        1.81147724e-01,  1.21222302e-01, -2.90186644e-01,  3.78536791e-01,\n",
       "        4.64347899e-01, -1.48983926e-01,  2.85754383e-01, -2.71713864e-02,\n",
       "       -4.92614537e-01,  2.60839760e-01,  5.22860229e-01,  3.59065801e-01,\n",
       "        2.38559186e-01,  4.40682501e-01,  1.07566737e-01, -1.88060105e-01,\n",
       "       -3.73944640e-01, -5.12474954e-01, -5.69389343e-01, -1.58000752e-01,\n",
       "       -4.45800275e-01, -2.86927164e-01, -4.59160693e-02,  3.74816924e-01,\n",
       "       -2.13803262e-01,  3.65021765e-01, -2.84619600e-01,  5.60481131e-01,\n",
       "       -1.04762107e-01, -4.84636009e-01, -5.11694886e-03,  1.64516315e-01,\n",
       "        3.58667195e-01,  2.91540504e-01,  1.42651960e-01,  8.39676142e-01,\n",
       "        4.62217569e-01, -1.53821632e-01,  4.71657723e-01,  1.56859547e-01,\n",
       "        4.73473996e-01, -6.88133478e-01,  5.03089428e-02, -2.71538943e-01,\n",
       "       -7.08665699e-02,  2.45833471e-01,  2.11413037e-02,  4.40854877e-01,\n",
       "       -3.16936314e-01, -2.32549861e-01, -2.25859314e-01, -5.08087635e-01,\n",
       "        4.59328383e-01, -9.73774195e-02, -5.36044165e-02,  1.62570253e-01,\n",
       "       -4.76750284e-02, -1.37404248e-01, -4.49380726e-01, -3.76121253e-02,\n",
       "        3.75530630e-01,  1.47776783e-01, -4.03004795e-01, -5.08713603e-01,\n",
       "        1.55387327e-01, -4.56035227e-01,  3.57727185e-02,  9.62296128e-02,\n",
       "        3.39618087e-01,  1.89585328e-01, -4.53946412e-01,  6.01785406e-02,\n",
       "        9.53614265e-02, -1.99645728e-01,  5.07244542e-02,  8.70361775e-02,\n",
       "        9.54785645e-02,  1.64917663e-01,  9.97716095e-03,  4.36995119e-01,\n",
       "       -4.81019258e-01,  9.31439102e-02, -4.53660250e-01, -2.05459639e-01,\n",
       "        3.07868093e-01, -1.25649884e-01, -7.72065520e-02, -1.80941537e-01,\n",
       "       -1.45098895e-01, -7.04361618e-01,  1.37775868e-01,  1.43234804e-01,\n",
       "        1.47204518e-01,  4.45376784e-01,  2.03408375e-01,  1.55002907e-01,\n",
       "        1.88002530e-02, -3.67500484e-01, -1.40905797e-01, -3.18972230e-01,\n",
       "        3.70829217e-02, -9.21655297e-02, -1.83553532e-01,  3.15664649e-01,\n",
       "       -3.29358667e-01, -1.44338787e-01, -5.05698562e-01,  4.16787788e-02,\n",
       "       -2.93838024e-01, -8.55298061e-03,  2.70332955e-02,  1.73095390e-01,\n",
       "       -2.02616140e-01,  2.38237798e-01,  1.70597225e-01, -4.31003422e-01,\n",
       "       -3.91709339e-03,  6.02219962e-02, -5.55183470e-01,  4.68674093e-01,\n",
       "       -5.67561947e-02, -2.37067938e-01,  3.98420215e-01,  4.24067751e-02,\n",
       "        2.45269150e-01, -5.99076506e-03, -1.45641327e-01,  1.00542337e-01,\n",
       "        1.54929832e-01, -1.78232282e-01, -4.64596719e-01,  8.87807533e-02,\n",
       "       -5.05230539e-02, -8.24210346e-01,  1.58499237e-02, -5.87338544e-02,\n",
       "       -4.90055978e-01,  5.52627165e-03,  1.49204269e-01, -2.60247178e-02,\n",
       "       -1.89193070e-01,  3.77895385e-01, -2.51294933e-02, -2.95399837e-02,\n",
       "        3.42127025e-01,  1.09741963e-01, -4.75208074e-01,  2.36611754e-01,\n",
       "        9.01743472e-02,  1.83028430e-01, -3.80632877e-01,  2.89924830e-01,\n",
       "       -7.17655644e-02, -3.76658961e-02, -1.49640599e-02, -4.52584654e-01,\n",
       "       -1.86854050e-01, -6.41925186e-02, -1.54172376e-01, -3.38533968e-02,\n",
       "        1.62960783e-01, -5.68726420e-01,  2.49448866e-01, -3.24599981e-01,\n",
       "        3.26850712e-01, -3.22593570e-01, -7.47667030e-02, -2.56547809e-01,\n",
       "       -8.27657819e-01,  7.72511423e-01,  5.09863198e-01, -1.47995070e-01,\n",
       "        4.96595055e-02, -2.46182144e-01,  2.82967776e-01,  2.09531844e-01,\n",
       "       -1.83223000e+01,  1.27236456e-01,  1.60959840e-01,  6.64408281e-02,\n",
       "        6.79389313e-02, -8.92197788e-02, -4.05106246e-02,  6.42991289e-02,\n",
       "        6.17698133e-02, -1.26665205e-01,  5.26150644e-01, -4.63486850e-01,\n",
       "        1.48110881e-01,  1.11219071e-01, -3.11272979e-01, -4.29273069e-01,\n",
       "        4.33289930e-02, -5.74534774e-01,  4.04145360e-01,  2.84657627e-01,\n",
       "        4.94403280e-02,  2.15974793e-01,  2.89201170e-01, -1.48725927e-01,\n",
       "        1.12026915e-01,  2.02226505e-01,  5.21053970e-01,  7.82107562e-02,\n",
       "        1.93452135e-01, -2.25502774e-01, -7.18049705e-02, -5.43017209e-01,\n",
       "        2.71206170e-01, -2.77324971e-02, -6.55057073e-01, -3.15454990e-01,\n",
       "       -2.95983344e-01,  1.02397665e-01, -1.70329362e-02, -8.42258893e-03,\n",
       "        4.38404113e-01, -1.88356668e-01, -1.97277501e-01, -5.58061004e-02,\n",
       "        9.20302607e-03, -7.15114251e-02,  1.23803653e-01,  9.76755768e-02,\n",
       "        2.43275240e-01,  3.01553249e-01,  4.79007900e-01,  1.00474291e-01,\n",
       "        2.19059587e-01, -3.68124336e-01, -3.60360324e-01,  2.76099741e-01,\n",
       "       -1.87256917e-01,  3.01068693e-01, -2.16489822e-01,  2.54276127e-01,\n",
       "       -4.45768721e-02, -2.97907125e-02, -3.68100405e-01,  1.68570831e-01,\n",
       "        5.87685592e-03, -2.94410646e-01, -1.84353352e-01,  4.81987894e-02,\n",
       "        4.39835228e-02, -6.87809736e-02, -3.96899283e-01, -4.66928221e-02,\n",
       "        6.84776604e-02, -1.00212455e+00,  5.56162931e-02,  2.06656650e-01,\n",
       "       -3.60997058e-02, -3.36277872e-01, -1.24448659e-02,  4.21645314e-01,\n",
       "       -1.28105775e-01, -2.32714310e-01, -4.18632120e-01, -3.66083652e-01,\n",
       "        2.93666810e-01, -3.09351504e-01, -1.68100484e-02,  2.41885602e-01,\n",
       "       -3.08981836e-01, -6.13000356e-02,  4.34591889e-01,  6.00321889e-02,\n",
       "        3.73872101e-01, -1.87342390e-01,  6.53541684e-01,  5.02179742e-01,\n",
       "        5.07888675e-01, -3.24798763e-01, -6.25169516e-01,  2.89446145e-01,\n",
       "       -4.97280031e-01, -1.50102928e-01,  8.23328644e-03,  1.56924129e-01,\n",
       "       -3.09853200e-02, -1.61887571e-01, -7.89992332e-01,  5.03455475e-02,\n",
       "        3.61113757e-01,  3.04001477e-02,  5.51582992e-01,  4.99456227e-02,\n",
       "        3.61632168e-01, -9.14105177e-02, -3.13085616e-01, -5.82084179e-01,\n",
       "        2.08037406e-01, -1.16526615e-02,  8.54914784e-02, -5.59990168e-01,\n",
       "       -1.23764947e-02,  1.63227096e-01, -5.54145992e-01,  7.14040846e-02,\n",
       "       -3.72947544e-01, -9.34602618e-02,  9.44139287e-02,  9.49143693e-02,\n",
       "        4.24656644e-02, -2.71738023e-01, -2.11972609e-01,  6.34715736e-01,\n",
       "       -5.21348715e-01,  3.56082827e-01, -1.14658490e-01,  5.68998121e-02,\n",
       "       -1.00280896e-01,  1.91977933e-01, -1.77876681e-01, -2.34762747e-02,\n",
       "        2.64369518e-01,  3.66591185e-01, -5.39533198e-01,  1.48976922e-01,\n",
       "       -2.05351606e-01,  4.33814675e-02, -2.06977755e-01,  2.49136358e-01,\n",
       "       -3.60929102e-01, -7.26573393e-02, -3.61479856e-02,  2.60594249e-01,\n",
       "        2.39738002e-01,  6.09425008e-01, -2.15549201e-01,  3.88824880e-01,\n",
       "       -9.42246392e-02,  2.17226729e-01, -2.22867519e-01, -5.43757200e-01,\n",
       "       -1.05390601e-01,  1.74474806e-01, -1.18792228e-01, -4.53070939e-01,\n",
       "        4.36121762e-01,  1.04041137e-01, -1.65331349e-01, -1.81021824e-01,\n",
       "       -7.45826364e-01,  2.91996181e-01,  1.57401292e-03,  3.46628353e-02,\n",
       "        3.62403803e-02, -1.46031991e-01, -5.04405424e-02, -3.05974215e-01,\n",
       "       -2.78999537e-01, -1.41515568e-01,  1.25549033e-01,  1.19728625e-01,\n",
       "       -1.02237225e-01, -2.24222213e-01,  3.42748076e-01, -3.40949476e-01,\n",
       "       -4.45965141e-01,  3.57420832e-01, -4.55860980e-03, -1.09096587e-01,\n",
       "       -5.28874472e-02,  1.22202471e-01, -6.48327231e-01,  2.41650641e-01,\n",
       "       -1.33629784e-01, -2.71540433e-01, -3.70661840e-02, -3.86332065e-01,\n",
       "        3.94754708e-01, -2.20705658e-01,  6.32253230e-01,  6.13961399e-01,\n",
       "       -3.87137115e-01,  1.57360569e-01, -1.90214410e-01,  1.05482340e-01,\n",
       "        2.73766875e-01, -8.33077580e-02,  2.50379175e-01,  5.21546483e-01,\n",
       "       -2.29300380e-01, -3.76100987e-01, -2.69154876e-01, -2.28329211e-01,\n",
       "       -1.09772228e-01, -1.64909884e-01,  3.52965802e-01, -1.34360954e-01,\n",
       "        1.30462870e-01, -3.44900675e-02,  1.57760233e-02,  4.58717406e-01,\n",
       "       -3.16447049e-01, -5.52562058e-01,  2.03367412e-01, -6.35454953e-02,\n",
       "        1.43577233e-01,  8.29437152e-02, -4.27595586e-01,  3.06106329e-01,\n",
       "       -2.08105847e-01,  1.43690929e-01, -1.28682017e-01, -3.94145936e-01,\n",
       "       -6.39282018e-02,  1.13435030e-01,  4.49050307e-01,  7.29421750e-02,\n",
       "        3.10380340e-01,  5.07208586e-01,  1.70312002e-01,  1.26001120e-01,\n",
       "       -2.03778446e-01, -1.67066768e-01,  9.65546165e-03,  2.07446188e-01,\n",
       "       -6.99926093e-02, -3.77005994e-01,  2.92200327e-01,  2.12911159e-01,\n",
       "        6.39125556e-02,  1.17832810e-01,  4.57733780e-01, -4.86869216e-01,\n",
       "       -7.52594948e-01, -2.82134116e-01,  1.69198558e-01, -4.71999645e-01,\n",
       "        8.75128284e-02,  3.53996567e-02,  2.58211106e-01,  2.61638343e-01,\n",
       "       -6.68676555e-01,  2.26831898e-01, -4.93766576e-01, -1.86323449e-01,\n",
       "       -5.80991916e-02,  9.52363387e-02, -4.41171944e-01, -1.89022839e-01,\n",
       "        2.69098967e-01, -1.59057435e-02,  3.59500982e-02, -2.51530081e-01,\n",
       "       -2.62587480e-02, -5.13728894e-02, -2.94971347e-01,  2.37062603e-01,\n",
       "       -2.63030171e-01, -5.45410454e-01, -3.99926931e-01,  1.93912759e-02,\n",
       "       -3.20884064e-02,  4.66094762e-01, -1.88208763e-02, -2.69131988e-01,\n",
       "       -3.89401346e-01,  4.60175499e-02, -3.14088136e-01, -2.23906748e-02,\n",
       "       -7.03930199e-01, -8.16360861e-02, -9.91650522e-02,  1.72406107e-01,\n",
       "        1.63220569e-01,  1.50593281e-01,  1.67218834e-01,  5.94109535e-01,\n",
       "        3.90080929e-01,  7.75964558e-02, -3.73394430e-01,  1.46181121e-01,\n",
       "       -4.30102110e-01,  1.20996192e-01,  4.76956517e-02, -1.40692264e-01,\n",
       "       -8.65423158e-02,  2.50029564e-01, -1.22958748e-02,  6.14898086e-01,\n",
       "        1.30832180e-01,  5.93555450e-01, -1.10216551e-01, -1.19122369e-02,\n",
       "        8.51760328e-01,  2.99260974e-01, -2.66833872e-01,  4.39341664e-01,\n",
       "       -1.13857247e-01, -4.94624883e-01, -5.42308427e-02,  7.56923333e-02,\n",
       "        3.44280124e-01, -1.62795946e-01,  9.77767929e-02, -3.64624590e-01,\n",
       "        6.82477206e-02, -1.45815760e-01, -2.48906314e-01,  3.30842227e-01,\n",
       "        2.94760287e-01,  1.69660360e-01,  6.42582029e-03,  4.71728861e-01,\n",
       "        3.96615535e-01, -1.12528995e-01, -2.54359037e-01, -2.83672094e-01,\n",
       "        3.02062720e-01, -1.32212654e-01,  4.42414075e-01,  2.94468105e-01,\n",
       "        1.21766433e-01, -7.20782205e-02, -3.39875698e-01, -2.92525619e-01,\n",
       "       -1.89353019e-01,  2.73457170e-01, -2.41174370e-01, -7.23730549e-02,\n",
       "        6.91488832e-02,  3.09255123e-01, -1.91981971e-01, -3.61350507e-01,\n",
       "       -1.38134509e-01, -1.88890636e-01,  5.10626316e-01,  3.63482684e-01,\n",
       "        2.47133896e-02,  3.69699061e-01,  3.68476212e-01,  6.61514163e-01,\n",
       "        1.02441812e+00,  1.82997540e-01, -2.26496071e-01, -6.42515779e-01,\n",
       "       -1.04738720e-01,  1.85829744e-01,  5.92340052e-01,  4.76812944e-02,\n",
       "        1.73593000e-01, -6.32628575e-02,  1.14314482e-01,  2.81246275e-01,\n",
       "       -4.38797742e-01, -1.07995428e-01,  2.16354668e-01,  3.56217653e-01,\n",
       "       -2.98834033e-02,  3.83604944e-01, -2.02973247e-01,  2.52639830e-01,\n",
       "       -1.23557068e-01, -1.47867680e-01, -1.22693874e-01, -1.65840939e-01,\n",
       "        2.90254951e-01, -5.75085104e-01,  3.82012188e-01, -6.04162633e-01,\n",
       "       -1.38076976e-01, -1.79547206e-01, -3.01174164e-01, -3.59287918e-01,\n",
       "       -6.62706137e-01,  3.72731596e-01,  2.41458580e-01, -3.65144372e-01,\n",
       "        1.01024583e-01,  2.07388904e-02, -1.36021048e-01,  8.74113590e-02,\n",
       "        2.33260557e-01, -2.20201582e-01,  3.29984426e-01,  9.94610846e-01,\n",
       "       -3.81564438e-01,  4.55839753e-01, -3.81658003e-02, -3.14570218e-01,\n",
       "       -7.87771463e-01, -4.76504624e-01,  3.75115663e-01,  6.88304365e-01,\n",
       "        6.18013963e-02,  1.16711117e-01, -2.82953292e-01,  4.07913215e-02,\n",
       "        5.39168045e-02,  1.36282807e-02,  2.24169627e-01, -1.36858985e-01,\n",
       "       -7.19727352e-02, -3.07178289e-01, -3.18574935e-01,  2.57633597e-01,\n",
       "       -4.84967828e-02, -2.53152430e-01,  2.80239522e-01,  1.77788436e-01,\n",
       "        8.71495903e-02,  1.70427680e-01,  1.99157968e-01, -1.45029470e-01,\n",
       "        2.11684510e-01,  2.57575989e-01,  3.21730301e-02, -3.71195853e-01,\n",
       "       -1.56524420e-01, -4.27634299e-01, -2.00111613e-01,  2.00177170e-03,\n",
       "        1.11238174e-01,  6.49568915e-01, -3.65678847e-01, -3.08431685e-01,\n",
       "        5.58511615e-01, -7.91894123e-02,  1.02438852e-01, -9.24135596e-02,\n",
       "        1.55006154e-02,  5.37300944e-01, -4.97027516e-01, -4.86677922e-02,\n",
       "       -1.35428026e-01, -5.36277927e-02, -3.83025825e-01,  1.84012309e-01,\n",
       "        1.63799450e-01, -2.88009077e-01, -5.35463989e-02, -1.34804681e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the sentence embedding to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./embeddings.npy to s3://sagemaker-us-east-1-757967535041/sagemaker/nlp-data-drift-bert-model/embeddings/embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp embeddings.npy s3://{bucket}/{model_prefix}/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Script\n",
    "\n",
    "Amazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage. \n",
    "\n",
    "Under the hood, when you create a MonitoringSchedule, Model Monitor ultimately kicks off processing jobs. Hence the container needs to be aware of the processing job contract \n",
    "\n",
    "We need to create an evaluation script that is compatible with container contract inputs and outputs\n",
    "\n",
    "[Container Contract Inputs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-inputs.html)\n",
    "\n",
    "[Container Contract Outputs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-outputs.html)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"Custom Model Monitoring script for Detecting Data Drift in NLP using SageMaker Model Monitor\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Python Built-Ins:\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m defaultdict\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SimpleNamespace\r\n",
      "\r\n",
      "\u001b[37m# External Dependencies:\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mscipy\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mspatial\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistance\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m cosine\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizer, BertModel\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_environment\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Load configuration variables for SM Model Monitoring job\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-inputs.html\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/processingjobconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m conffile:\r\n",
      "            defaults = json.loads(conffile.read())[\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        traceback.print_exc()\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnable to read environment vars from SM processing config file\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        defaults = {}\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m SimpleNamespace(\r\n",
      "        dataset_format=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mdataset_format\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mdataset_format\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\r\n",
      "        dataset_source=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset_source\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mdataset_source\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/endpoint\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        ),\r\n",
      "        end_time=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mend_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mend_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\r\n",
      "        output_path=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33moutput_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/resultdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        ),\r\n",
      "        publish_cloudwatch_metrics=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mpublish_cloudwatch_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mpublish_cloudwatch_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mEnabled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        ),\r\n",
      "        sagemaker_endpoint_name=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker_endpoint_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker_endpoint_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        ),\r\n",
      "        sagemaker_monitoring_schedule_name=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker_monitoring_schedule_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker_monitoring_schedule_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\r\n",
      "        ),\r\n",
      "        start_time=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mstart_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mstart_time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\r\n",
      "        max_ratio_threshold=\u001b[36mfloat\u001b[39;49;00m(os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mTHRESHOLD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \r\n",
      "             defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mTHRESHOLD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnan\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))),\r\n",
      "        bucket=os.environ.get(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            defaults.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)),\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_embeddings_file\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    env = get_environment()\r\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36ms3fs\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcore\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m S3FileSystem\r\n",
      "    s3 = S3FileSystem()\r\n",
      "    \r\n",
      "    key = \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker/nlp-data-drift-bert-model/embeddings/embeddings.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    bucket = env.bucket\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mS3 bucket name is\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,bucket)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m np.load(s3.open(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket, key)))\r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    env = get_environment()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting evaluation with config\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00menv\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDownloading Embedding File\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m#download BERT embedding file used for fine-tuning BertForSequenceClassification\u001b[39;49;00m\r\n",
      "    embedding_list = download_embeddings_file()\r\n",
      "    \r\n",
      "    \u001b[37m# Load pre-trained model tokenizer (vocabulary)\u001b[39;49;00m\r\n",
      "    tokenizer = BertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Load pre-trained model (weights)\u001b[39;49;00m\r\n",
      "    model = BertModel.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                      output_hidden_states = \u001b[34mTrue\u001b[39;49;00m, \u001b[37m# Whether the model returns all hidden-states.\u001b[39;49;00m\r\n",
      "                                      )\r\n",
      "\r\n",
      "    \u001b[37m# Put the model in \"evaluation\" mode, meaning feed-forward operation.\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "    \r\n",
      "    \u001b[37m# Tokenize all of the sentences and map the tokens to thier word IDs.\u001b[39;49;00m\r\n",
      "    sent_cosine_dict = {}\r\n",
      "    violations = []\r\n",
      "    \r\n",
      "    total_record_count = \u001b[34m0\u001b[39;49;00m  \u001b[37m# Including error predictions that we can't read the response for\u001b[39;49;00m\r\n",
      "    error_record_count = \u001b[34m0\u001b[39;49;00m\r\n",
      "    counts = defaultdict(\u001b[36mint\u001b[39;49;00m)  \u001b[37m# dict defaulting to 0 when unseen keys are requested\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m path, directories, filenames \u001b[35min\u001b[39;49;00m os.walk(env.dataset_source):\r\n",
      "        \u001b[34mfor\u001b[39;49;00m filename \u001b[35min\u001b[39;49;00m \u001b[36mfilter\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m f: f.lower().endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), filenames):\r\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(path, filename), \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\r\n",
      "                \u001b[34mfor\u001b[39;49;00m entry \u001b[35min\u001b[39;49;00m file:\r\n",
      "                    total_record_count += \u001b[34m1\u001b[39;49;00m\r\n",
      "                    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "                        response = json.loads(json.loads(entry)[\u001b[33m\"\u001b[39;49;00m\u001b[33mcaptureData\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mendpointInput\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "                    \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "                        \u001b[34mcontinue\u001b[39;49;00m\r\n",
      "                \r\n",
      "                    \u001b[34mfor\u001b[39;49;00m record \u001b[35min\u001b[39;49;00m response:\r\n",
      "                        encoded_dict = tokenizer.encode_plus(\r\n",
      "                            record, \r\n",
      "                            add_special_tokens = \u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            max_length = \u001b[34m64\u001b[39;49;00m,\r\n",
      "                            padding= \u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            return_attention_mask = \u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            return_tensors = \u001b[33m'\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            truncation=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            )\r\n",
      "\r\n",
      "                        \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "                            outputs = model(encoded_dict[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], encoded_dict[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "                            hidden_states = outputs[\u001b[34m2\u001b[39;49;00m]\r\n",
      "                            token_vecs = hidden_states[-\u001b[34m2\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\r\n",
      "                            input_sentence_embedding = torch.mean(token_vecs, dim=\u001b[34m0\u001b[39;49;00m)\r\n",
      "                        \r\n",
      "                        cosine_score = \u001b[34m0\u001b[39;49;00m\r\n",
      "                        \r\n",
      "                        \u001b[34mfor\u001b[39;49;00m embed_item \u001b[35min\u001b[39;49;00m embedding_list:\r\n",
      "                            cosine_score += (\u001b[34m1\u001b[39;49;00m - cosine(input_sentence_embedding, embed_item))\r\n",
      "                        cosine_score_avg = cosine_score/(\u001b[36mlen\u001b[39;49;00m(embedding_list))\r\n",
      "                        \u001b[34mif\u001b[39;49;00m cosine_score_avg < env.max_ratio_threshold:\r\n",
      "                            error_record_count += \u001b[34m1\u001b[39;49;00m\r\n",
      "                            sent_cosine_dict[record] = cosine_score_avg\r\n",
      "                            violations.append({\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33msentence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record,\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33mavg_cosine_score\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: cosine_score_avg,\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33mfeature_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33msent_cosine_score\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33mconstraint_check_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mbaseline_drift_check\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoint_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : env.sagemaker_endpoint_name,\r\n",
      "                                    \u001b[33m\"\u001b[39;49;00m\u001b[33mmonitoring_schedule_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: env.sagemaker_monitoring_schedule_name\r\n",
      "                                })\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mChecking for constraint violations...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mViolations: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mviolations \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(violations) \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting violations file...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(env.output_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mconstraints_violations.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m outfile:\r\n",
      "        outfile.write(json.dumps(\r\n",
      "            { \u001b[33m\"\u001b[39;49;00m\u001b[33mviolations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: violations },\r\n",
      "            indent=\u001b[34m4\u001b[39;49;00m,\r\n",
      "        ))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting overall status output...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/message\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m outfile:\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(violations):\r\n",
      "            msg = \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "            \u001b[34mfor\u001b[39;49;00m v \u001b[35min\u001b[39;49;00m violations:\r\n",
      "                msg += \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCompletedWithViolations: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mv[\u001b[33m'\u001b[39;49;00m\u001b[33msentence\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "                msg +=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            msg = \u001b[33m\"\u001b[39;49;00m\u001b[33mCompleted: Job completed successfully with no violations.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        outfile.write(msg)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(msg)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\r\n",
      "    \u001b[37m#if env.publish_cloudwatch_metrics:\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting CloudWatch metrics...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/output/metrics/cloudwatch/cloudwatch_metrics.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33ma+\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m outfile:\r\n",
      "            \u001b[37m# One metric per line (JSONLines list of dictionaries)\u001b[39;49;00m\r\n",
      "            \u001b[37m# Remember these metrics are aggregated in graphs, so we report them as statistics on our dataset\u001b[39;49;00m\r\n",
      "            outfile.write(json.dumps(\r\n",
      "            { \u001b[33m\"\u001b[39;49;00m\u001b[33mviolations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: violations },\r\n",
      "            indent=\u001b[34m4\u001b[39;49;00m,\r\n",
      "            ))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize docker/evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Push Image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'nlp-data-drift-bert-v1'\n",
    "tag = ':latest'\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  17.41kB\n",
      "Step 1/9 : FROM python:3.7-slim-buster\n",
      " ---> b5900b90787e\n",
      "Step 2/9 : RUN pip3 install sagemaker\n",
      " ---> Using cache\n",
      " ---> fccd9f2a7991\n",
      "Step 3/9 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> c247162d8522\n",
      "Step 4/9 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> c0fa548d853f\n",
      "Step 5/9 : RUN pip3 install torch\n",
      " ---> Using cache\n",
      " ---> 5d959178e287\n",
      "Step 6/9 : RUN pip3 install s3fs\n",
      " ---> Using cache\n",
      " ---> c31974fc2b7a\n",
      "Step 7/9 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 9fa91339885b\n",
      "Step 8/9 : ADD evaluation.py /\n",
      " ---> b64cf3d05d5f\n",
      "Step 9/9 : ENTRYPOINT [\"python3\", \"/evaluation.py\"]\n",
      " ---> Running in 567a3862a0bd\n",
      "Removing intermediate container 567a3862a0bd\n",
      " ---> cacf6ab9c602\n",
      "Successfully built cacf6ab9c602\n",
      "Successfully tagged nlp-data-drift-bert-v1:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'nlp-data-drift-bert-v1' already exists in the registry with id '757967535041'\n",
      "The push refers to repository [757967535041.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-v1]\n",
      "\n",
      "\u001b[1Bfbb025b5: Preparing \n",
      "\u001b[1B2fc41f44: Preparing \n",
      "\u001b[1Bc5c5d2d2: Preparing \n",
      "\u001b[1B97037c38: Preparing \n",
      "\u001b[1Be40ed307: Preparing \n",
      "\u001b[1Bad741945: Preparing \n",
      "\u001b[1B5de3cd48: Preparing \n",
      "\u001b[1B531659d0: Preparing \n",
      "\u001b[1Bf98ac977: Preparing \n",
      "\u001b[1B4a515eac: Preparing \n",
      "\u001b[11Bbb025b5: Pushed lready exists 8kBA\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2Klatest: digest: sha256:1618dc47517c20339d3c4b315d0a7e24b31cf2de3be2c46930cfb01ca2086ba2 size: 2640\n"
     ]
    }
   ],
   "source": [
    "# Creating the ECR repository and pushing the container image\n",
    "\n",
    "# SageMaker Classic Notebook Instance:\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri\n",
    "\n",
    "# SageMaker Studio:\n",
    "# !cd docker && sm-docker build . --repository $ecr_repository$tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Monitor for detetcing data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "monitor = ModelMonitor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5', 'bucket': bucket },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-757967535041/sagemaker/CustomModelMonitor/nlp-data-drift-bert-endpoint/monitoring_schedule\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session.default_bucket()}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "\n",
    "processing_output = ProcessingOutput(\n",
    "    output_name='result',\n",
    "    source='/opt/ml/processing/resultdata',\n",
    "    destination=destination,\n",
    ")\n",
    "output = MonitoringOutput(source=processing_output.source, destination=processing_output.destination)\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='nlp-data-drift-bert-schedule',\n",
    "    output=output,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:757967535041:monitoring-schedule/nlp-data-drift-bert-schedule',\n",
       " 'MonitoringScheduleName': 'nlp-data-drift-bert-schedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'DataQuality',\n",
       " 'CreationTime': datetime.datetime(2021, 8, 12, 21, 49, 11, 175000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 8, 12, 21, 49, 15, 594000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinition': {'MonitoringInputs': [{'EndpointInput': {'EndpointName': 'nlp-data-drift-bert-endpoint',\n",
       "      'LocalPath': '/opt/ml/processing/input/endpoint',\n",
       "      'S3InputMode': 'File',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}}],\n",
       "   'MonitoringOutputConfig': {'MonitoringOutputs': [{'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-757967535041/sagemaker/CustomModelMonitor/nlp-data-drift-bert-endpoint/monitoring_schedule',\n",
       "       'LocalPath': '/opt/ml/processing/resultdata',\n",
       "       'S3UploadMode': 'Continuous'}}]},\n",
       "   'MonitoringResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.large',\n",
       "     'VolumeSizeInGB': 30}},\n",
       "   'MonitoringAppSpecification': {'ImageUri': '757967535041.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-v1:latest'},\n",
       "   'StoppingCondition': {'MaxRuntimeInSeconds': 3600},\n",
       "   'Environment': {'THRESHOLD': '0.5',\n",
       "    'bucket': 'sagemaker-us-east-1-757967535041'},\n",
       "   'RoleArn': 'arn:aws:iam::757967535041:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole'},\n",
       "  'MonitoringType': 'DataQuality'},\n",
       " 'EndpointName': 'nlp-data-drift-bert-endpoint',\n",
       " 'ResponseMetadata': {'RequestId': '91001884-9dbe-4da2-bbc3-7d30cee1a6a9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '91001884-9dbe-4da2-bbc3-7d30cee1a6a9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1422',\n",
       "   'date': 'Thu, 12 Aug 2021 21:49:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: nlp-data-drift-bert-schedule\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = monitor.list_executions()\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No processing job has been executed yet. \n",
      "    This means that one hour has not passed yet. \n",
      "    You can go to the next code cell and run the processing job manually\n"
     ]
    }
   ],
   "source": [
    "if len(jobs) > 0:\n",
    "    last_execution_desc = monitor.list_executions()[-1].describe()\n",
    "    print(last_execution_desc)\n",
    "    print(f'\\nExit Message: {last_execution_desc.get(\"ExitMessage\", \"None\")}')\n",
    "else:\n",
    "    print(\"\"\"No processing job has been executed yet. \n",
    "    This means that one hour has not passed yet. \n",
    "    You can go to the next code cell and run the processing job manually\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually execute the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  nlp-data-drift-bert-v1-2021-08-12-21-51-29-927\n",
      "Inputs:  [{'InputName': 'endpointdata', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-757967535041/sagemaker/CustomModelMonitor/datacapture/nlp-data-drift-bert-endpoint', 'LocalPath': '/opt/ml/processing/input/endpoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-757967535041/sagemaker/CustomModelMonitor/nlp-data-drift-bert-endpoint/monitoring_schedule', 'LocalPath': '/opt/ml/processing/resultdata', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".............................\u001b[34mStarting evaluation with config\u001b[0m\n",
      "\u001b[34mnamespace(bucket='sagemaker-us-east-1-757967535041', dataset_format=None, dataset_source='/opt/ml/processing/input/endpoint', end_time=None, max_ratio_threshold=0.5, output_path='/opt/ml/processing/resultdata', publish_cloudwatch_metrics='Enabled', sagemaker_endpoint_name=None, sagemaker_monitoring_schedule_name=None, start_time=None)\u001b[0m\n",
      "\u001b[34mDownloading Embedding File\u001b[0m\n",
      "\u001b[34mS3 bucket name is sagemaker-us-east-1-757967535041\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|| 232k/232k [00:00<00:00, 50.6MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|| 28.0/28.0 [00:00<00:00, 41.7kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|| 466k/466k [00:00<00:00, 52.9MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|| 570/570 [00:00<00:00, 790kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   1%|         | 5.89M/440M [00:00<00:07, 58.9MB/s]#015Downloading:   3%|         | 12.4M/440M [00:00<00:06, 62.8MB/s]#015Downloading:   4%|         | 18.8M/440M [00:00<00:06, 63.3MB/s]#015Downloading:   6%|         | 25.3M/440M [00:00<00:06, 64.0MB/s]#015Downloading:   7%|         | 31.9M/440M [00:00<00:06, 64.5MB/s]#015Downloading:   9%|         | 38.3M/440M [00:00<00:06, 63.9MB/s]#015Downloading:  10%|         | 44.9M/440M [00:00<00:06, 64.5MB/s]#015Downloading:  12%|        | 51.4M/440M [00:00<00:06, 64.8MB/s]#015Downloading:  13%|        | 57.9M/440M [00:00<00:05, 64.6MB/s]#015Downloading:  15%|        | 64.5M/440M [00:01<00:05, 65.0MB/s]#015Downloading:  16%|        | 71.0M/440M [00:01<00:05, 65.0MB/s]#015Downloading:  18%|        | 77.5M/440M [00:01<00:05, 64.9MB/s]#015Downloading:  19%|        | 84.1M/440M [00:01<00:05, 65.3MB/s]#015Downloading:  21%|        | 90.7M/440M [00:01<00:05, 64.9MB/s]#015Downloading:  22%|       | 97.3M/440M [00:01<00:05, 65.3MB/s]#015Downloading:  24%|       | 104M/440M [00:01<00:05, 65.5MB/s] #015Downloading:  25%|       | 110M/440M [00:01<00:05, 65.6MB/s]#015Downloading:  27%|       | 117M/440M [00:01<00:04, 65.0MB/s]#015Downloading:  28%|       | 124M/440M [00:01<00:04, 65.2MB/s]#015Downloading:  30%|       | 130M/440M [00:02<00:04, 65.4MB/s]#015Downloading:  31%|       | 137M/440M [00:02<00:04, 62.4MB/s]#015Downloading:  32%|      | 143M/440M [00:02<00:04, 62.7MB/s]#015Downloading:  34%|      | 149M/440M [00:02<00:04, 62.8MB/s]#015Downloading:  35%|      | 156M/440M [00:02<00:04, 62.8MB/s]#015Downloading:  37%|      | 162M/440M [00:02<00:04, 62.9MB/s]#015Downloading:  38%|      | 168M/440M [00:02<00:04, 63.1MB/s]#015Downloading:  40%|      | 175M/440M [00:02<00:04, 63.2MB/s]#015Downloading:  41%|      | 181M/440M [00:02<00:04, 63.2MB/s]#015Downloading:  43%|     | 187M/440M [00:02<00:04, 63.3MB/s]#015Downloading:  44%|     | 194M/440M [00:03<00:03, 63.4MB/s]#015Downloading:  45%|     | 200M/440M [00:03<00:03, 63.4MB/s]#015Downloading:  47%|     | 206M/440M [00:03<00:03, 63.3MB/s]#015Downloading:  48%|     | 213M/440M [00:03<00:03, 63.5MB/s]#015Downloading:  50%|     | 219M/440M [00:03<00:03, 63.0MB/s]#015Downloading:  51%|     | 226M/440M [00:03<00:03, 63.3MB/s]#015Downloading:  53%|    | 232M/440M [00:03<00:03, 63.5MB/s]#015Downloading:  54%|    | 238M/440M [00:03<00:03, 63.6MB/s]#015Downloading:  56%|    | 245M/440M [00:03<00:03, 63.9MB/s]#015Downloading:  57%|    | 251M/440M [00:03<00:02, 63.8MB/s]#015Downloading:  58%|    | 258M/440M [00:04<00:02, 63.7MB/s]#015Downloading:  60%|    | 264M/440M [00:04<00:02, 63.7MB/s]#015Downloading:  61%|   | 270M/440M [00:04<00:02, 63.7MB/s]#015Downloading:  63%|   | 277M/440M [00:04<00:02, 63.6MB/s]#015Downloading:  64%|   | 283M/440M [00:04<00:02, 63.3MB/s]#015Downloading:  66%|   | 289M/440M [00:04<00:02, 63.5MB/s]#015Downloading:  67%|   | 296M/440M [00:04<00:02, 62.6MB/s]#015Downloading:  69%|   | 302M/440M [00:04<00:02, 62.2MB/s]#015Downloading:  70%|   | 308M/440M [00:04<00:02, 62.5MB/s]#015Downloading:  71%|  | 315M/440M [00:04<00:02, 62.8MB/s]#015Downloading:  73%|  | 321M/440M [00:05<00:01, 63.0MB/s]#015Downloading:  74%|  | 327M/440M [00:05<00:01, 63.2MB/s]#015Downloading:  76%|  | 334M/440M [00:05<00:01, 62.8MB/s]#015Downloading:  77%|  | 340M/440M [00:05<00:01, 62.8MB/s]#015Downloading:  79%|  | 346M/440M [00:05<00:01, 58.0MB/s]#015Downloading:  80%|  | 352M/440M [00:05<00:01, 59.0MB/s]#015Downloading:  81%| | 359M/440M [00:05<00:01, 59.8MB/s]#015Downloading:  83%| | 365M/440M [00:05<00:01, 61.0MB/s]#015Downloading:  84%| | 372M/440M [00:05<00:01, 62.2MB/s]#015Downloading:  86%| | 378M/440M [00:05<00:00, 63.2MB/s]#015Downloading:  87%| | 385M/440M [00:06<00:00, 63.9MB/s]#015Downloading:  89%| | 391M/440M [00:06<00:00, 64.4MB/s]#015Downloading:  90%| | 398M/440M [00:06<00:00, 64.2MB/s]#015Downloading:  92%|| 404M/440M [00:06<00:00, 63.9MB/s]#015Downloading:  93%|| 411M/440M [00:06<00:00, 63.4MB/s]#015Downloading:  95%|| 417M/440M [00:06<00:00, 63.5MB/s]#015Downloading:  96%|| 423M/440M [00:06<00:00, 63.4MB/s]#015Downloading:  98%|| 430M/440M [00:06<00:00, 63.4MB/s]#015Downloading:  99%|| 436M/440M [00:06<00:00, 62.8MB/s]#015Downloading: 100%|| 440M/440M [00:06<00:00, 63.4MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mChecking for constraint violations...\u001b[0m\n",
      "\u001b[34mViolations: [{'sentence': 'Thanks so much for driving me home', 'avg_cosine_score': 0.36653403077832636, 'feature_name': 'sent_cosine_score', 'constraint_check_type': 'baseline_drift_check', 'endpoint_name': None, 'monitoring_schedule_name': None}, {'sentence': 'Thanks so much for cooking dinner. I really appreciate it', 'avg_cosine_score': 0.34974954400044717, 'feature_name': 'sent_cosine_score', 'constraint_check_type': 'baseline_drift_check', 'endpoint_name': None, 'monitoring_schedule_name': None}, {'sentence': 'Nice to meet you, Sergio. So, where are you from', 'avg_cosine_score': 0.3789828703767611, 'feature_name': 'sent_cosine_score', 'constraint_check_type': 'baseline_drift_check', 'endpoint_name': None, 'monitoring_schedule_name': None}]\u001b[0m\n",
      "\u001b[34mWriting violations file...\u001b[0m\n",
      "\u001b[34mWriting overall status output...\u001b[0m\n",
      "\u001b[34mCompletedWithViolations: Thanks so much for driving me home\u001b[0m\n",
      "\u001b[34mCompletedWithViolations: Thanks so much for cooking dinner. I really appreciate it\u001b[0m\n",
      "\u001b[34mCompletedWithViolations: Nice to meet you, Sergio. So, where are you from\n",
      "\u001b[0m\n",
      "\u001b[34mWriting CloudWatch metrics...\u001b[0m\n",
      "\u001b[34mDone\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5','bucket': bucket },\n",
    ")\n",
    "    \n",
    "processor.run(\n",
    "    [ProcessingInput(\n",
    "        input_name='endpointdata',\n",
    "        source = \"s3://{}/{}/{}\".format(bucket, data_capture_prefix,endpoint_name),\n",
    "        #source=f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture',\n",
    "        destination = '/opt/ml/processing/input/endpoint',\n",
    "    )],\n",
    "    [ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/resultdata',\n",
    "        destination=destination,\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Lastly, please remember to delete the monitoring schedule and Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: nlp-data-drift-bert-schedule\n"
     ]
    }
   ],
   "source": [
    "#Delete the monitoring schedule\n",
    "monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f82487a1-a635-44ef-a895-3ed7df1f4a20',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f82487a1-a635-44ef-a895-3ed7df1f4a20',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 12 Aug 2021 21:59:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
